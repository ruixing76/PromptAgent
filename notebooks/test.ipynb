{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89bf2057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# conda environments:\n",
      "#\n",
      "base                   /home/rui.xing/miniconda3\n",
      "prompt_agent         * /home/rui.xing/miniconda3/envs/prompt_agent\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b48ec133-ec85-4811-8dfb-4bf04c737e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "019856b7-e70f-4c11-a947-4d7ddc326dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Set\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "410540ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_json(data, file_path, is_friendly_format=True, is_verbose=False):\n",
    "    if is_friendly_format:\n",
    "        indent = 4\n",
    "    else:\n",
    "        indent = None\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=indent)\n",
    "    if is_verbose:\n",
    "        print(f\"Data is saved to {file_path}\")\n",
    "\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    with jsonlines.open(file_path, 'r') as reader:\n",
    "        data = [d for d in reader]\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_jsonl(file_path, data):\n",
    "    with jsonlines.open(file_path, 'w') as writer:\n",
    "        for d in data:\n",
    "            writer.write(d)\n",
    "    print(f\"Data is saved to {file_path}\")\n",
    "\n",
    "# Transform data to have one entry per claim-note pair\n",
    "def flatten_data(data):\n",
    "    flattened_data=[]\n",
    "    for item in data:\n",
    "        claim = item[\"claim\"]\n",
    "        for note in item[\"notes\"]:\n",
    "            flattened_data.append({\n",
    "                \"claim\": claim,\n",
    "                \"note_text\": note[\"text\"],\n",
    "                \"reasons\": note.get(\"reasons\", \"\"),\n",
    "                \"label\": note[\"label\"]  # Use the label associated with the note\n",
    "            })\n",
    "    return flattened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a655cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine train, val and test data\n",
    "train_data = read_jsonl('../datasets/notes_en_train.jsonl')\n",
    "val_data = read_jsonl('../datasets/notes_en_val.jsonl')\n",
    "test_data = read_jsonl('../datasets/notes_en_test.jsonl')\n",
    "# flatten the data\n",
    "flattened_train_data = flatten_data(train_data)\n",
    "flattened_val_data = flatten_data(val_data)\n",
    "flattened_test_data = flatten_data(test_data)\n",
    "\n",
    "combined_data= {'train': flattened_train_data, 'eval': flattened_val_data, 'test': flattened_test_data}\n",
    "\n",
    "write_json(combined_data,'../datasets/notes_en_combined.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d0ba9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'eval', 'test'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9df2579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.community_notes import CustomTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f08fe492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will appear in the options\n",
    "REASON_LABELS = {\n",
    "    'helpfulAddressesClaim': 'a',\n",
    "    'helpfulClear': 'b',\n",
    "    'helpfulEmpathetic': 'c',\n",
    "    'helpfulGoodSources': 'd',\n",
    "    'helpfulImportantContext': 'e',\n",
    "    'helpfulInformative': 'f',\n",
    "    'helpfulUnbiasedLanguage': 'g',\n",
    "    'helpfulUniqueContext': 'h',\n",
    "    'notHelpfulArgumentativeOrBiased': 'i',\n",
    "    'notHelpfulHardToUnderstand': 'j',\n",
    "    'notHelpfulIncorrect': 'k',\n",
    "    'notHelpfulIrrelevantSources': 'l',\n",
    "    'notHelpfulMissingKeyPoints': 'm',\n",
    "    'notHelpfulNoteNotNeeded': 'n',\n",
    "    'notHelpfulOffTopic': 'o',\n",
    "    'notHelpfulOpinionSpeculation': 'p',\n",
    "    'notHelpfulOpinionSpeculationOrBias': 'q',\n",
    "    'notHelpfulOther': 'r',\n",
    "    'notHelpfulSourcesMissingOrUnreliable': 's',\n",
    "    'notHelpfulSpamHarassmentOrAbuse': 't',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84bac19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_response(response):\n",
    "    letters = ''.join(REASON_LABELS.values())\n",
    "    clean_pattern = r\"<answer>([\\s\\S]*?)<\\/answer>\"\n",
    "    match = re.findall(clean_pattern, response.lower())\n",
    "    if len(match) == 0 or not match[-1].strip():\n",
    "        pattern_str = '|'.join([re.escape(option) for option in REASON_LABELS])\n",
    "        backup_match = re.findall(pattern_str, response, re.IGNORECASE)\n",
    "\n",
    "        if backup_match:\n",
    "            return REASON_LABELS[backup_match[-1].lower()]\n",
    "        else:\n",
    "            return 'N/A: Format error'\n",
    "\n",
    "    # Extract all valid option letters (upper or lower case), separated by semicolon, comma, or whitespace\n",
    "    answer_section = match[-1]\n",
    "    found_letters = re.findall(r\"[\" + letters + \"]\", answer_section)\n",
    "    if not found_letters:\n",
    "        return 'N/A: Format error'\n",
    "    # Take at most two letters, uppercase, join with semicolon\n",
    "    # result = ';'.join([l.lower() for l in found_letters[:2]])\n",
    "    result = set(found_letters)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59fa9079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_correct(preds:List[Set], labels:List[Set]) -> List[int]:\n",
    "    '''\n",
    "    <task specific>\n",
    "    The function of comparing the predictions and labels in community notes task, input are list of sets.\n",
    "\n",
    "    preds: List of sets, each set contains the predicted labels for a claim-note pair.\n",
    "    labels: List of sets, each set contains the true labels for a claim-note pair.\n",
    "    Returns a list of integers, where 1 indicates a correct prediction and 0 indicates an incorrect prediction.\n",
    "    '''\n",
    "    comparisons = []\n",
    "    for p, l in zip(preds, labels):\n",
    "        # compute the intersection of predicted and true labels, comparison = intersection of p and l // union of p and l\n",
    "        intersection = p.intersection(l)\n",
    "        union = p.union(l)\n",
    "        if len(union) == 0:\n",
    "            # if both p and l are empty, we consider it a correct prediction\n",
    "            comparisons.append(1)\n",
    "        elif len(intersection) > 0:\n",
    "            # if the intersection is not empty, it means the prediction is correct\n",
    "            comparisons.append(len(intersection) / len(union))\n",
    "        else:\n",
    "            # if the intersection is empty, it means the prediction is incorrect\n",
    "            comparisons.append(0)\n",
    "    return comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0672c803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_labels(labels):\n",
    "    '''\n",
    "    <task specific>\n",
    "    Transfer the form of the task ground-truth answers to List(set) \n",
    "    or List(str) that fit the input requirement of function \"cal_correct\"\n",
    "    \n",
    "    Do nothing if the data is alreadly loaded that way.\n",
    "    '''\n",
    "    # turn labels that separated by semicolon into a set, according to REASON_LABELS\n",
    "    cleaned_labels = []\n",
    "    for label in labels:\n",
    "        reason_1=REASON_LABELS[label.split(';')[0]]\n",
    "        reason_2=REASON_LABELS[label.split(';')[1]]\n",
    "        cleaned_labels.append(set([reason_1, reason_2]))\n",
    "    return cleaned_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67717996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'m', 'n'}, {'a', 'd'}, {'j', 'q'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test examples for clean_labels\n",
    "labels = [\"notHelpfulNoteNotNeeded;notHelpfulMissingKeyPoints\",\n",
    "         \"helpfulAddressesClaim;helpfulGoodSources\",\n",
    "         \"notHelpfulOpinionSpeculationOrBias;notHelpfulHardToUnderstand\"]\n",
    "clean_labels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc3c1b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 0.6666666666666666]\n"
     ]
    }
   ],
   "source": [
    "# test examples for cal_correct\n",
    "preds = [set(['a', 'b']), set(['c','d']), set(['d', 'e'])]\n",
    "labels = [set(['a', 'b']), set(['c', 'd']), set(['d', 'e', 'f'])]\n",
    "# Test the cal_correct function\n",
    "print(cal_correct(preds, labels))  # Expected output: [1.0, 1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d7b4dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'd'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_response('<answer>(a);(d)</answer>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219e294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
